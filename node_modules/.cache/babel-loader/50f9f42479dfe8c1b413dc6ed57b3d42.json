{"ast":null,"code":"'use strict';\n\nconst errCode = require('err-code');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst persist = require('../../utils/persist');\n\nconst {\n  DAGNode,\n  DAGLink\n} = require('ipld-dag-pb');\n\nconst parallelBatch = require('it-parallel-batch');\n\nconst mh = require('multihashing-async').multihash;\n/**\n * @typedef {import('../../types').BlockAPI} BlockAPI\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\n\n\nconst dagBuilders = {\n  flat: require('./flat'),\n  balanced: require('./balanced'),\n  trickle: require('./trickle')\n};\n/**\n * @param {File} file\n * @param {BlockAPI} block\n * @param {ImporterOptions} options\n */\n\nasync function* buildFileBatch(file, block, options) {\n  let count = -1;\n  let previous;\n  let bufferImporter;\n\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter;\n  } else {\n    bufferImporter = require('./buffer-importer');\n  }\n\n  for await (const entry of parallelBatch(bufferImporter(file, block, options), options.blockWriteConcurrency)) {\n    count++;\n\n    if (count === 0) {\n      previous = entry;\n      continue;\n    } else if (count === 1 && previous) {\n      yield previous;\n      previous = null;\n    }\n\n    yield entry;\n  }\n\n  if (previous) {\n    previous.single = true;\n    yield previous;\n  }\n}\n/**\n * @param {File} file\n * @param {BlockAPI} block\n * @param {ImporterOptions} options\n */\n\n\nconst reduce = (file, block, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer(leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0];\n\n      if (leaf.cid.codec === 'raw' && (file.mtime !== undefined || file.mode !== undefined)) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let {\n          data: buffer\n        } = await block.get(leaf.cid, options);\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        });\n        const multihash = mh.decode(leaf.cid.multihash);\n        buffer = new DAGNode(leaf.unixfs.marshal()).serialize();\n        leaf.cid = await persist(buffer, block, { ...options,\n          codec: 'dag-pb',\n          hashAlg: multihash.name,\n          cidVersion: options.cidVersion\n        });\n        leaf.size = buffer.length;\n      }\n\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      };\n    } // create a parent node and add all the leaves\n\n\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    });\n    const links = leaves.filter(leaf => {\n      if (leaf.cid.codec === 'raw' && leaf.size) {\n        return true;\n      }\n\n      if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n        return true;\n      }\n\n      return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length);\n    }).map(leaf => {\n      if (leaf.cid.codec === 'raw') {\n        // node is a leaf buffer\n        f.addBlockSize(leaf.size);\n        return new DAGLink('', leaf.size, leaf.cid);\n      }\n\n      if (!leaf.unixfs || !leaf.unixfs.data) {\n        // node is an intermediate node\n        f.addBlockSize(leaf.unixfs && leaf.unixfs.fileSize() || 0);\n      } else {\n        // node is a unixfs 'file' leaf node\n        f.addBlockSize(leaf.unixfs.data.length);\n      }\n\n      return new DAGLink('', leaf.size, leaf.cid);\n    });\n    const node = new DAGNode(f.marshal(), links);\n    const buffer = node.serialize();\n    const cid = await persist(buffer, block, options);\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    };\n  }\n\n  return reducer;\n};\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\n\n\nfunction fileBuilder(file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy];\n\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY');\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options);\n}\n\nmodule.exports = fileBuilder;","map":null,"metadata":{},"sourceType":"script"}